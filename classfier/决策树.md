# 机器学习复习

## 决策树

### 基本思想

基于特征进行分类

每次分裂前找到最优划分特征(ID3：信息增益，C4.5信息增益+信息增益率，cart基尼系数)



### 相关概念

信息熵：反应散度，信息熵越小，样本越集中。越大越好

信息增益：划分后分支结点的加权平均信息熵与父节点信息熵的差值

信息增益率：信息增益/特征的信息熵。避免过拟合。



基尼系数：随机抽两个样本，取值不同的概率$1-\Sigma _{k=1}^K p_k^2$,越小越好





### 剪枝Pruning

#### 预剪枝

分裂前后的误差之差超过设定阈值才分裂

##### 优点：简单

##### 缺点：容易欠拟合



#### 后剪枝

##### REP错误率预剪枝

由下至上遍历所有子节点，尝试将该结点替换为子树中数量最多的样本，直至没有可以让模型在**剪枝数据集**中的表现提升的结点。

###### 优点：简单、线性复杂度

###### 缺点：若剪枝数据集很小，容易过拟合



##### PEP悲观剪枝

**自上而下**。 根据剪枝前后的错误率来判定是否进行子树的修剪，因此不需要单独的剪枝数据集。 

 对于一个叶子节点，它覆盖了 n个样本，其中有 e 个错误，那么该叶子节点的错误率为 $(e+0.5)/n$，这里的 0.5 为惩罚因子 (惩罚因子一般设置为 0.5)。 

 对于一个有L 个叶子节点的子树，其误判率为： 

![](https://quailwwk1.oss-cn-beijing.aliyuncs.com/typora截图/20200424163222.png)



**假设错误次数服从伯努利分布**

![](https://quailwwk1.oss-cn-beijing.aliyuncs.com/typora截图/20200424163317.png)

![](https://quailwwk1.oss-cn-beijing.aliyuncs.com/typora截图/20200424163330.png)

###### 优点：精度较高，不需额外的剪枝数据集，线性复杂度

###### 缺点：自上而下，容易导致欠拟合





##### [CCP代价复杂度剪枝](https://www.zhihu.com/question/22697086)

随着alpha增大，

对于原始的CART树A0，先剪去一棵子树，生成子树A1，然后再从A1剪去一棵子树生成A2，直到最后剪到只剩一个根结点的子树An。于是得到了A0-AN一共n+1棵子树。然后再用n+1棵子树预测独立的验证数据集，谁的误差最小就选谁。



如何选择减去哪棵子树？

![](https://quailwwk1.oss-cn-beijing.aliyuncs.com/typora截图/20200424170432.png)

分子为剪枝前后的**剪枝数据集**误差差别，分母为剪枝后叶节点减少的数量。

某棵子树剪枝前后的g(t)<alpha，则要剪。

随着alpha的增大，越来越多的子树需要被剪。

###### 缺点：复杂度高，为二次





### 常用算法

#### ID3

##### 使用信息增益

##### 优点：简单

##### 缺点：信息增益倾向于选择取值较多的特征

#### C4.5

##### 信息增益率

##### 悲观剪枝

##### 连续值处理

对样本中连续特征的取值升序排序，取各个取值之间的中点作为候选分裂点，选择信息增益最高的分裂点。

要减去一个与连续值取值个数有关的修正，否则会倾向于选择连续特征。

##### 缺点：因为要处理连续值，计算效率较低

#### CART

##### gini系数

##### 可以做回归

散度指标：一阶：LAD.二阶：样本方差

##### CCP剪枝



##### 缺点：CCP剪枝复杂度高







