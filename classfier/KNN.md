# 思路

给定测试样本，找出训练集中与其距离最短的k个训练样本，基于这k个样本的信息进行预测。

具体地，分类任务可使用投票法，即k个样本中最多的标签；回归任务可用平均法

 ![img](https://pic4.zhimg.com/80/v2-e780b42d95a9d577c264fa1183b571ef_720w.jpg) 

# 特点

懒惰学习(lazy learning)：训练阶段仅仅是把样本保存起来，训练时间开销为0



# 调参--K

K越大，分类越模糊，越容易欠拟合

k越小，越迁就于训练集，容易过拟合



# 改进--加权KNN

 **加权KNN做的事情就是，比较 最近的K个点的距离d 的单调递减函数f(d)的函数值**

 ![img](https://pic3.zhimg.com/80/v2-9f37dbfd2c60650a50234a7e6249ac32_720w.jpg) 

 在点Y的预测中，改范围内三角形分类数量占优，因此将Y点归为三角形。但是从视觉上观测，应该是分为圆形分类更为合理。

## 距离倒数加权

 最简单地，我们有$f(d)=\frac 1d$。但是，想到当d趋向于零时，该函数值$f(d)=\frac 1d \to \infin$。这相当于为距离很近的点分配了无限大的权重，假如恰巧有一个噪声的点在离要预测的点非常近的位置，那么这一噪音对于预测结果的干扰就非常大了。 

## 高斯函数加权

 考虑高斯函数 ![[公式]](https://www.zhihu.com/equation?tex=f%28d%29+%3D+exp%28-d%5E2%2F2%29) 

 ![img](https://pic3.zhimg.com/80/v2-95badc26b71be6910d0a395a207f3de2_720w.jpg) 

 这个函数就很好地弥补了反比例函数的缺点，当d趋向于零时，函数值趋向于1。当d>0时，f(d)单调递减（显然距离d取正值）。同时高斯函数还有一个优点就是，当距离d很大时，函数值f(d)始终会保持正值，而不会跌入负值。 





# 优缺点

## 优点

- 简单，易于理解，无需建模与训练，易于实现；
- 适合对稀有事件进行分类；
- 适合与多分类问题，例如根据基因特征来判断其功能分类，kNN比SVM的表现要好。
-  与朴素贝叶斯一样支持“增量式训练”：只需要不断添加新的训练数据而不需要重复训练模型 

## 缺点

- 对每一个个体的预测都是全局搜索，数据量特别大时计算量也会太大，影响效率
- 对于样本量分布不均衡的时候，会产生误分（重视多数类而忽视少数类）。
-  预测需要以来存储的训练数据集，存储成本较高 



# 改进查找

## [KD-Tree](https://zhuanlan.zhihu.com/p/45346117)

### 建立

\1. 建立根节点；

\2. 选取[**方差最大**](方差越大，数据越分散，分割之后的区分度越高)的特征作为分割特征；

\3. 选择该特征的中位数作为分割点；

\4. 将数据集中该特征小于中位数的传递给根节点的左儿子，大于中位数的传递给根节点的右儿子；

\5. 递归执行步骤2-4，直到所有数据都被建立到KD Tree的节点上为止。



### 查找

\1. 从根节点开始，根据目标在分割特征中是否小于或大于当前节点，向左或向右移动。

\2. 一旦算法到达叶节点，它就将节点点保存为“当前最佳”。

\3. 回溯，即从叶节点再返回到根节点

\4. 如果当前节点比当前最佳节点更接近，那么它就成为当前最好的。



