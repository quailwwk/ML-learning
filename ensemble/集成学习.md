# Bagging

## 自助法抽样

每轮都使用自助法(有放回抽样)对训练集进行抽样

始终不被采到的概率=$\lim_{m\to \infin}(1-\frac 1m)^m=\frac 1e=0.368 $

适用于样本少，难以有效划分训练集及测试集的情景





## 弱而不同

对于同一基学习器，利用训练样本的不同，训练出尽可能独立的基学习器，再用投票法(分类)/取均值(回归)组合结果。

## 降低方差

防止单个弱分类器过拟合





# Boosting

## 初始化样本权重分布

## 训练弱分类器

### 用上一轮的样本权重分布训练弱分类器

### 计算误差率

 Gm(x)在训练数据集上的**误差率**em就是被Gm(x)误分类样本的权值之和。 

### 根据错误率计算本轮弱分类器的权重

![](https://quailwwk1.oss-cn-beijing.aliyuncs.com/typora截图/20200424173426.png)

### 根据错误率调整样本权重

错分类样本的权重上升

### 将加权弱分类器加到前一轮组合的分类器中





## 缺点

### 异常样本的权重增加

### 不可并行计算





# bagging和boosting区别

## 1.样本使用

bagging使用部分样本，boosting使用全部样本

## 2.弱分类器的权重

bagging等权，boosting会调整

## 3.样本权重

## 4.并行计算



# stacking堆叠法

## 层状结构

以常见的两层stacking为例，第一层是不同的基模型(多个)，而第二层是以上一层基模型输出结果为输入数据的分类器(一个)。从某种意义上看，和神经网络的“结构相似”。

## 使用上一层的输出训练

## 与深度学习联系

### 区别

#### stacking关注宽度(分每层类器数量)，深度学习关注网络深度

### 共同问题

#### 1.黑箱的不可解释性

#### 2.过拟合

## 与bagging区别

## stacking准而不同，bagging弱而不同





