

# ANN人工神经网络

## BP反向传播算法

- **输入训练集**

- ***\*对于训练集中的每个样本x，设置输入层（Input layer）对应的激活值\**![img](https://img-blog.csdn.net/20160401203715973)：**
  - **前向传播：**

![img](https://img-blog.csdn.net/20160401203739849)， ![img](https://img-blog.csdn.net/20160401203806927)

- - **计算输出层产生的错误：**

![img](https://img-blog.csdn.net/20160401203820380)

- - **反向传播错误：**

![img](https://img-blog.csdn.net/20160401203833583)

- **使用梯度下降（gradient descent），训练参数：**

  ​													 ![img](https://img-blog.csdn.net/20160401203848037)

![img](https://img-blog.csdn.net/20160401203859349)

其中：$\eta$为学习率，x为不同样本



# 调参--学习率

学习率过大，容易不收敛

学习率过小，收敛会很慢





# 梯度下降

## 批量梯度下降batch GD

每次更新都遍历全部样本

![img](https://pic4.zhimg.com/80/v2-5809743fd06c4ff804753d29e4b83935_720w.jpg)

## 随机梯度下降SGD

每次更新只用一个样本，遍历整租样本之后再次循环

![img](https://pic3.zhimg.com/80/v2-b3f14a09ad27df9c66a3af208060f5d7_720w.jpg)

**对于最优化问题，凸问题，**虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近。但是相比于批量梯度，这样的方法更快，**更快收敛**，虽然不是全局最优，但很多时候是我们**可以接受**的，所以这个方法用的也比上面的多。



**注意SGD的学习率要逐渐减低，否则到后期会不稳定**



SGDClassifier

SGDRegressor

#### SGD的优点是：

- 高效
- 容易实现（有许多机会进行代码调优）

#### SGD的缺点是：

- SGD需要许多超参数：比如正则项参数、迭代数。
- SGD对于特征归一化（feature scaling）是敏感的。



## minibatch GD

折中，用的是最多

注意一次循环中每次用的样本不一样，要遍历整个样本．

![img](https://pic3.zhimg.com/80/v2-96181aa7bc39a7e5fb54cfdf2c42a7b9_720w.jpg)





## 早期停止法--防止过拟合

等验证误差超过最小值一段时间之后再停止（这时你可以确信模型不会变得更好了），然后将模型参数回滚到验证误差最小时的位置。

![](https://quailwwk1.oss-cn-beijing.aliyuncs.com/typora截图/20200307144641.png)





# 激活函数

| 名称                 |                                                              | 压缩范围     | 优点                                             | 缺点                                                         |
| -------------------- | ------------------------------------------------------------ | ------------ | ------------------------------------------------ | ------------------------------------------------------------ |
| **sigmoid**          | ![img](https://pic2.zhimg.com/80/v2-dc07a3592b3698404dfb92828b906ab9_720w.png) | [0,1]        |                                                  | 1.梯度消失(饱和神经元0 1)<br />2.不以零为中心<br />3.exp函数计算成本高 |
| **tanh**             | ![这里写图片描述](https://img-blog.csdn.net/20180422215001607?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2Vkb2dhd2FjaGlh/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70) | [-1,1]       | 以零为中心                                       | 梯度消失（饱和神经元-1,1）                                   |
| **ReLU修正线性单元** | ![img](https://pic4.zhimg.com/80/v2-eb6cd472b7eb49ae738aff37d0d8e4af_720w.png) | [0,$\infin$] | 阈值化，计算效率高                               | 不以零为中心、dead_relu(x<0)                                 |
| **Leaky ReLU**       | ![img](https://pic2.zhimg.com/80/v2-2002238573e7d29a558858fb77eec829_720w.png) | R            | 缓解dead ReLU问题                                |                                                              |
| **指数线性单元**     | **![](https://quailwwk1.oss-cn-beijing.aliyuncs.com/typora截图/20200426173058.png)** |              | **对relu的改进，能够缓解死亡relu问题。**         |                                                              |
| **swish**            | ![](https://quailwwk1.oss-cn-beijing.aliyuncs.com/typora截图/20200421221905.png) |              | 用swish替代relu将获得轻微效果提升。              |                                                              |
| **Softmax**          | ![img](https://pic2.zhimg.com/80/v2-e02ac0cc66db2a8396c30af116d11b49_720w.jpg) | [0,1]        | 可以处理多分类问题<br />最好在分类器的输出层使用 |                                                              |

- 用于**分类器**时，Sigmoid函数及其组合通常效果更好。
- 由于梯度消失问题，有时要避免使用sigmoid和tanh函数。
- ReLU函数是一个通用的激活函数，目前在大多数情况下使用。
- 如果神经网络中出现死神经元，那么PReLU函数就是最好的选择。
- 请记住，**ReLU函数只能在隐藏层中使用**。





# LSTM

![img](https://pic4.zhimg.com/80/v2-e4f9851cad426dfe4ab1c76209546827_720w.jpg)

网络输入值：$x_t$

LSTM隐藏层状态：$h_{t-1}$

模型输出值：$y_t$

细胞状态：$c_{t-1}$

当前输入新信息：$\tilde c_t \in (-1,1)$

遗忘向量：$f_t\in (0,1)$，用于控制前一时刻细胞信息有多少传递到当前时刻细胞状态

输入向量：$i_t\in (0,1)$，用于控制当前的新信息有多少传递到当前的细胞状态

输出向量：$o_t\in (0,1)$，用于控制细胞状态的信息有多少传递到输出值中

sigmoid层：处理为(0,1)

tanh层：处理为（-1,1）



## 结构

![](https://quailwwk1.oss-cn-beijing.aliyuncs.com/typora截图/20200324020049.png)

每个LSTM层要更新两个变量：$c_t$及隐藏层状态$h_t$

其中$c_t$=来自t-1时刻的影响([遗忘门](##遗忘门(forget gate)))+t时刻的影响([输入门](##输入门（input gate）))

$h_t$ = $F(c_t)$([输出门](##输出门（output gate）))



### 遗忘门

决定前一时刻的细胞状态$c_{t-1}$有多少信息传递到到当前时刻$c_t$中，由$f_t \in (0,1) $控制

![遗忘门](https://img-blog.csdn.net/20170516234108350)



其中$W_f = [W_{fh}, W_{fx}]$

### 输入门

控制当前输入新生成的信息$\tilde{c_t} \in (-1,1)$有多少可以加入到细胞$c_t$中，由$i_t \in (0,1)$控制

tanh 层用来产生当前时刻新的信息，sigmoid 层用来控制有多少新信息可以传递给细胞状态。

 ![输入门](https://img-blog.csdn.net/20170516234139929)

### 更新细胞状态

基于[遗忘门](##遗忘门(forget gate))及[输入门](##输入门（input gate）)的结果，更新当前时刻细胞状态$c_t$

 更新后的细胞状态有两部分构成，一，来自上一时刻旧的细胞状态信息 $C_{t−1}$；二，当前输入新生成的信息 $C_t$。

![更新细胞状态](https://img-blog.csdn.net/20170516234203376)

其中*为各个元素相乘。

### 输出门

控制有多少细胞状态信息可以传递到输出中去，由sigmoid层$o_t\in (0,1)$控制。

$o_t=\sigma(W_o[h_{t-1}, x_t]^T+b_o)$

![](https://quailwwk1.oss-cn-beijing.aliyuncs.com/typora截图/20200324021839.png)

### 更新输出值

基于[更新的细胞状态](##更新细胞状态)及输出门，输出隐藏状态$h_t$，其中细胞状态$c_t$经$tanh$层缩放至$(-1,1)$

![输出门](https://img-blog.csdn.net/20170516234227283)





## BPTT算法

### 1.固定某一层，沿时间t的反向传播

![](https://quailwwk1.oss-cn-beijing.aliyuncs.com/typora截图/20200426182638.png)

### 2.固定时刻t，传递到上一层



## RNN的梯度爆炸、梯度消失

反向传播时，梯度对时间有依赖，

 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7BL_%7Bt%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D%3D%5Csum_%7Bk%3D0%7D%5E%7Bt%7D%7B%5Cfrac%7B%5Cpartial%7BL_%7Bt%7D%7D%7D%7B%5Cpartial%7BO_%7Bt%7D%7D%7D%5Cfrac%7B%5Cpartial%7BO_%7Bt%7D%7D%7D%7B%5Cpartial%7BS_%7Bt%7D%7D%7D%7D%28%5Cprod_%7Bj%3Dk%2B1%7D%5E%7Bt%7D%7B%5Cfrac%7B%5Cpartial%7BS_%7Bj%7D%7D%7D%7B%5Cpartial%7BS_%7Bj-1%7D%7D%7D%7D%29%5Cfrac%7B%5Cpartial%7BS_%7Bk%7D%7D%7D%7B%5Cpartial%7BW_%7Bx%7D%7D%7D) 

又 ![[公式]](https://www.zhihu.com/equation?tex=S_%7Bj%7D%3Dtanh%28W_%7Bx%7DX_%7Bj%7D%2BW_%7Bs%7DS_%7Bj-1%7D%2Bb_%7B1%7D%29) 

故![](https://quailwwk1.oss-cn-beijing.aliyuncs.com/typora截图/20200426175126.png)

tanh的导数[0,1]，当WS很小时，梯度消失



**系数$W_s$（乘在上一个时刻的输出上的系数）是根源所在**

### LSTM的解决：

![](https://quailwwk1.oss-cn-beijing.aliyuncs.com/typora截图/20200325182033.png)

$W_C$的计算

