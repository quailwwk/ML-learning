# LDA

## 思想

 **LDA是一种监督学习的降维技术，**也就是说它的数据集的每个样本是有类别输出的，**这点和PCA不同。**  **PCA是不考虑样本类别输出的无监督降维技术。** 



 **LDA的思想可以用一句话概括，就是“投影后类内方差最小，类间方差最大”** 

![](https://quailwwk1.oss-cn-beijing.aliyuncs.com/typora截图/20200502173552.png)



## 原理

![](https://quailwwk1.oss-cn-beijing.aliyuncs.com/typora截图/20200502173629.png)

![](https://quailwwk1.oss-cn-beijing.aliyuncs.com/typora截图/20200502173648.png)

![](https://quailwwk1.oss-cn-beijing.aliyuncs.com/typora截图/20200502174006.png)

[奇异值分解](./数学基础/奇异值分解SVD.md)

![](https://quailwwk1.oss-cn-beijing.aliyuncs.com/typora截图/20200502175954.png)

## 多分类LDA

![](https://quailwwk1.oss-cn-beijing.aliyuncs.com/typora截图/20200502180220.png)



# [PCA](C:\我的坚果云\python学习\机器学习\PCA.md)

## 基本思想

​		从原始的空间中顺序地找一组相互正交的坐标轴，新的坐标轴的选择与数据本身是密切相关的。

​		其中，第一个新坐标轴选择是原始数据中方差最大的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴是与第1,2个轴正交的平面中方差最大的。依次类推，可以得到n个这样的坐标轴。

 		我们发现，大部分方差都包含在前面k个坐标轴中，后面的坐标轴所含的方差几乎为0。于是，我们可以忽略余下的坐标轴，只保留前面k个含有绝大部分方差的坐标轴。事实上，这相当于只保留包含绝大部分方差的维度特征，而忽略包含方差几乎为0的特征维度，实现对数据特征的降维处理。 



## 做法

​		通过计算数据矩阵的协方差矩阵，然后得到**协方差矩阵的特征值特征向量**，选择**特征值最大(即方差最大)的k个特征所对应的特征向量组成的矩阵**。这样就可以将数据矩阵转换到新的空间当中，实现数据特征的降维。 

​		

​		 得到协方差矩阵的特征值特征向量有两种方法：**特征值分解协方差矩阵、奇异值分解协方差矩阵**（若用奇异值分解，可以不求出协方差矩阵$XX^T$)







## 步骤 

设有m条n维数据。

1）将原始数据按列组成n行m列矩阵X

2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值

3）求出协方差矩阵$C=\frac{1}{m}XX^\mathsf{T}$

4）求出协方差矩阵的特征值及对应的特征向量(用特征值分解或奇异值分解)

5）将特征向量按对应特征值大小从上到下**按行**排列成矩阵，取前k行组成矩阵$P^T$（维数为k*n）

6）$Y=P^TX$即为降维到k维后的数据



## 原理

### 特征值分解

对协方差矩阵$\Sigma$做特征值分解

![](https://quailwwk1.oss-cn-beijing.aliyuncs.com/typora截图/20200502183938.png)

其中，

![](https://quailwwk1.oss-cn-beijing.aliyuncs.com/typora截图/20200502184001.png)

![](https://quailwwk1.oss-cn-beijing.aliyuncs.com/typora截图/20200502184014.png)

![](https://quailwwk1.oss-cn-beijing.aliyuncs.com/typora截图/20200502184118.png)

上面的操作没有降维，若要降维可以取U的前k列。

### [奇异值分解](./数学基础/奇异值分解SVD.md)

M*N矩阵A

​	  ![img](https://img-blog.csdn.net/20150123170018218) ![img](https://img-blog.csdn.net/20150123170122018) 

U：左奇异矩阵，$AA^T$的特征向量构成的矩阵

V：右奇异矩阵，$A^T A$的特征向量构成的矩阵

在线性代数上，**实对称矩阵有一系列非常好的性质：**

**1）实对称矩阵不同特征值对应的特征向量必然正交。**

**2）设特征向量$\lambda$重数为r，则必然存在r个线性无关的特征向量对应于$\lambda$，因此可以将这r个特征向量单位正交化。**

因此U、V都可化为正交矩阵



取A=X-EX，则U即为协方差矩阵$\Sigma$的特征向量矩阵

取P为U的前K列

令$Y=P^T(X-\bar X)$，则Y即为前K个主成分。



#### 特性--可以对样本大小进行压缩

左奇异矩阵：对特征维度进行压缩

右奇异矩阵：对样本大小进行压缩





## k的选取--方差占比

 ![img](https://img-blog.csdn.net/20180609150202272?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Byb2dyYW1fZGV2ZWxvcGVy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70) 

其中$x^{(i)}_{approx}$为PCA映射值。

分子表示均方映射误差。

该式表示因为做PCA而损失的方差占比， 比如t值取0.01，则代表了该PCA算法保留了99%的主要信息。 

上式还可以用SVD分解时产生的S矩阵来表示，如下面的式子：

![img](https://img-blog.csdn.net/20180609150234299?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Byb2dyYW1fZGV2ZWxvcGVy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)